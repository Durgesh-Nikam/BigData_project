# üåæ Smart Farming Decision Support System  
**Big Data | Kafka | Spark | ML | Airflow | Streaming**

---

## üìå Project Overview

The **Smart Farming Decision Support System** is a Big Data‚Äìdriven analytics platform designed to support agricultural decision-making using **batch processing**, **machine learning**, and **real-time data streaming**.

The system processes historical agricultural and rainfall datasets using **Apache Spark**, trains ML models using **Spark MLlib**, and applies these models on real-time farming data streamed via **Apache Kafka**. The entire pipeline is orchestrated using **Apache Airflow (Standalone)**.

---

## üß† One-Line Summary

Historical agricultural and rainfall datasets are processed using Spark batch jobs to train ML models, while real-time farming data is streamed through Kafka and analyzed using Spark Structured Streaming, orchestrated using Apache Airflow.

---

## üèóÔ∏è Architecture Overview
```bash
Historical CSV Data
‚îî‚îÄ‚îÄ Kafka Batch Producer
‚îî‚îÄ‚îÄ Kafka Topic
‚îî‚îÄ‚îÄ Spark Batch Processing
‚îî‚îÄ‚îÄ Data Enrichment (Rainfall + Soil)
‚îî‚îÄ‚îÄ ML Model Training (Spark MLlib)
‚îî‚îÄ‚îÄ Saved ML Model

Real-Time Data Generator
‚îî‚îÄ‚îÄ Kafka Stream Producer
‚îî‚îÄ‚îÄ Kafka Topic
‚îî‚îÄ‚îÄ Spark Structured Streaming
‚îî‚îÄ‚îÄ Apply Trained ML Model
‚îî‚îÄ‚îÄ Yield Predictions
‚îî‚îÄ‚îÄ Parquet Storage
```

---

## üìÇ Project Structure
```bash
BigData_project/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ historical/
‚îÇ ‚îú‚îÄ‚îÄ indian_agriculture.csv
‚îÇ ‚îî‚îÄ‚îÄ Monthly_Rainfall_From_1901_to_2017.csv
‚îÇ
‚îú‚îÄ‚îÄ kafka/
‚îÇ ‚îú‚îÄ‚îÄ batch_producer.py
‚îÇ ‚îî‚îÄ‚îÄ realtime_producer.py
‚îÇ
‚îú‚îÄ‚îÄ spark/
‚îÇ ‚îú‚îÄ‚îÄ batch_processing.py
‚îÇ ‚îú‚îÄ‚îÄ batch_enrichment.py
‚îÇ ‚îú‚îÄ‚îÄ model_training_enriched.py
‚îÇ ‚îî‚îÄ‚îÄ streaming_prediction_enriched.py
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ ‚îî‚îÄ‚îÄ smart_farming_yield_model_enriched
‚îÇ
‚îú‚îÄ‚îÄ storage/
‚îÇ ‚îú‚îÄ‚îÄ parquet/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ agriculture_all_crops/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ agriculture_ml_enriched/
‚îÇ ‚îî‚îÄ‚îÄ streaming_predictions/
‚îÇ
‚îú‚îÄ‚îÄ airflow/
‚îÇ ‚îî‚îÄ‚îÄ dags/
‚îÇ ‚îî‚îÄ‚îÄ smart_farming_dag.py
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üìä Datasets Used

1. **Indian Agriculture Dataset**
   - Crop area, production, and yield data

2. **Monthly Rainfall Dataset (1901‚Äì2017)**
   - State-wise rainfall data

> Soil parameters are simulated using rainfall-based indices for analytical enrichment.

---

## ‚öôÔ∏è System Requirements

- **OS**: Ubuntu 20.04 / 22.04  
- **Java**: OpenJDK 11  
- **Python**: 3.8+  
- **Apache Kafka**
- **Apache Spark**
- **Apache Airflow (Standalone)**

---

## üîß Installation & Setup

### 1Ô∏è‚É£ Install Java
```bash
sudo apt install openjdk-11-jdk -y
java -version
```
### 2Ô∏è‚É£ Setup Apache Kafka
```bash
tar -xzf kafka_2.13-3.x.x.tgz
export KAFKA_HOME=~/kafka_2.13-3.x.x
export PATH=$PATH:$KAFKA_HOME/bin
```

### 3Ô∏è‚É£ Start Kafka Services (Run in Separate Terminals)

Terminal 1 ‚Äì Zookeeper
```bash
zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties
```

Terminal 2 ‚Äì Kafka Broker
```bash
kafka-server-start.sh $KAFKA_HOME/config/server.properties
```

### 4Ô∏è‚É£ Create Kafka Topics (ONE TIME ONLY)

topic - 1
```bash
kafka-topics.sh --create \
--topic agriculture_batch \
--bootstrap-server localhost:9092 \
--partitions 1 \
--replication-factor 1
```
topic - 2
```bash
kafka-topics.sh --create \
--topic agriculture_stream \
--bootstrap-server localhost:9092 \
--partitions 1 \
--replication-factor 1
```


### 5Ô∏è‚É£ Verify Apache Spark
```bash
spark-submit --version
```

### 6Ô∏è‚É£ Start Apache Airflow (Standalone)
```bash
pip install apache-airflow
```
```bash
airflow standalone
```

- Web UI: http://localhost:8080
- Login credentials are printed on first run

---

## üöÄ How to Run the Project
### Step 1: Ensure Kafka is Running

(Zookeeper and Kafka broker must be active)

### Step 2: Start Airflow
```bash
airflow standalone
```

### Step 3: Deploy DAG
```bash
cp airflow/dags/smart_farming_dag.py ~/airflow/dags/
```

### Step 4: Trigger DAG

1. Open Airflow UI
2. Enable DAG: ```bash smart_farming_decision_support ``` 
3. Click Trigger DAG

---

## üîÑ DAG Execution Flow

1. Kafka batch producer sends historical data
2. Spark batch processing transforms data
3. Spark batch enrichment adds rainfall & soil features
4. Spark ML model is trained
5. Kafka real-time producer generates live data
6. Spark Structured Streaming applies ML model on live data
7. Real-time tasks run in parallel and stop gracefully after a fixed duration.

--- 

## üìà Outputs

- Trained ML model ‚Üí ```bash models/ ```
- Batch processed data ‚Üí ```bash storage/parquet/ ```
- Streaming predictions ‚Üí ```bash storage/streaming_predictions/ ```

---
